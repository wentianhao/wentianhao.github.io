<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="先逃跑再说"><meta name="copyright" content="先逃跑再说"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>Attention Is All You Need | wanheo</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.24/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_j5gk85dg4pf.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script>document.addEventListener("DOMContentLoaded", () => {
  Yun.utils.renderKatex();
});</script><link rel="icon" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"wentianhao.github.io","root":"/","title":"云游君的小站","version":"1.6.1","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><meta name="description" content="Introduction传统的基于RNN的Seq2Seq模型难以处理长序列的句子，无法实现并行，还面临对齐的问题，本文提出Attention机制，加入Attention的Seq2Seq模型在各个任务上都有显著提升，创新点在于抛弃传统的Encoder-Decoder模型必须结合CNN或RNN的固有模式，只用Attention机制。">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Is All You Need">
<meta property="og:url" content="https://wentianhao.github.io/2020/10/10/Attention/index.html">
<meta property="og:site_name" content="wanheo">
<meta property="og:description" content="Introduction传统的基于RNN的Seq2Seq模型难以处理长序列的句子，无法实现并行，还面临对齐的问题，本文提出Attention机制，加入Attention的Seq2Seq模型在各个任务上都有显著提升，创新点在于抛弃传统的Encoder-Decoder模型必须结合CNN或RNN的固有模式，只用Attention机制。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://whh.plus/images/attention.png">
<meta property="og:image" content="https://whh.plus/images/attention.jpg">
<meta property="og:image" content="https://whh.plus/images/theTransformer.png">
<meta property="og:image" content="https://whh.plus/images/Thetransformer1.png">
<meta property="og:image" content="https://whh.plus/images/Thetransformer2.png">
<meta property="og:image" content="https://whh.plus/images/encoder.png">
<meta property="og:image" content="https://whh.plus/images/decoder.png">
<meta property="og:image" content="https://whh.plus/images/embedding.png">
<meta property="og:image" content="https://whh.plus/images/encoder1.png">
<meta property="og:image" content="https://whh.plus/images/encoder2.png">
<meta property="og:image" content="https://whh.plus/images/attention1.png">
<meta property="og:image" content="https://whh.plus/images/QKV.png">
<meta property="og:image" content="https://whh.plus/images/QKV1.png">
<meta property="og:image" content="https://whh.plus/images/computeAttention.png">
<meta property="og:image" content="https://whh.plus/images/softmax.png">
<meta property="og:image" content="https://whh.plus/images/short-cut.png">
<meta property="og:image" content="https://whh.plus/images/short-cut1.png">
<meta property="og:image" content="https://whh.plus/images/transformer1.png">
<meta property="og:image" content="https://whh.plus/images/multi-head1.png">
<meta property="og:image" content="https://whh.plus/images/multi-head2.png">
<meta property="og:image" content="https://whh.plus/images/multiAttention.png">
<meta property="og:image" content="https://whh.plus/images/multi-head3.png">
<meta property="og:image" content="https://whh.plus/images/position.png">
<meta property="og:image" content="https://whh.plus/images/transformer.png">
<meta property="article:published_time" content="2020-10-10T01:59:03.000Z">
<meta property="article:modified_time" content="2020-10-10T01:59:03.000Z">
<meta property="article:author" content="先逃跑再说">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://whh.plus/images/attention.png"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="先逃跑再说"><img width="96" loading="lazy" src="/yun.png" alt="先逃跑再说"></a><div class="site-author-name"><a href="/about/">先逃跑再说</a></div><span class="site-name">wanheo</span><sub class="site-subtitle">sub</sub><div class="site-desciption">desc</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">58</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">22</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">31</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://qm.qq.com/cgi-bin/qm/qr?k=kZJzggTTCf4SpvEQ8lXWoi5ZjhAx0ILZ&amp;jump_from=webapi" title="QQ 群 1050458482" target="_blank" style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/YunYouJun" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://weibo.com/jizhideyunyoujun" title="微博" target="_blank" style="color:#E6162D"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-weibo-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.douban.com/people/yunyoujun/" title="豆瓣" target="_blank" style="color:#007722"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-douban-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=247102977" title="网易云音乐" target="_blank" style="color:#C20C0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/yunyoujun/" title="知乎" target="_blank" style="color:#0084FF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhihu-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/1579790" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/about/white-qrcode-and-search.jpg" title="微信公众号" target="_blank" style="color:#1AAD19"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-2-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://twitter.com/YunYouJun" title="Twitter" target="_blank" style="color:#1da1f2"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-twitter-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://t.me/elpsycn" title="Telegram Channel" target="_blank" style="color:#0088CC"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-telegram-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:me@yunyoujun.cn" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://travellings.now.sh/" title="Travelling" target="_blank" style="color:var(--hty-text-color)"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Background"><span class="toc-number">2.</span> <span class="toc-text">Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer"><span class="toc-number">3.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">整体结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E7%BC%96%E7%A0%81"><span class="toc-number">3.2.</span> <span class="toc-text">输入编码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E8%BF%87%E7%A8%8B"><span class="toc-number">3.2.1.</span> <span class="toc-text">编码过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-attention"><span class="toc-number">3.3.</span> <span class="toc-text">self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#self-attention%E7%BB%86%E8%8A%82"><span class="toc-number">3.3.1.</span> <span class="toc-text">self-attention细节</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE"><span class="toc-number">3.3.2.</span> <span class="toc-text">残差</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Head-Attention"><span class="toc-number">4.</span> <span class="toc-text">Multi-Head Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number">5.</span> <span class="toc-text">Positional Encoding</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://wentianhao.github.io/2020/10/10/Attention/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="先逃跑再说"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="wanheo"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Attention Is All You Need</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-10-10 09:59:03" itemprop="dateCreated datePublished" datetime="2020-10-10T09:59:03+08:00">2020-10-10</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/NLP/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">NLP</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><p><img src="https://whh.plus/images/attention.png" alt="attention" loading="lazy"></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>传统的基于RNN的Seq2Seq模型难以处理长序列的句子，无法实现并行，还面临对齐的问题，本文提出Attention机制，加入Attention的Seq2Seq模型在各个任务上都有显著提升，创新点在于抛弃传统的Encoder-Decoder模型必须结合CNN或RNN的固有模式，只用Attention机制。</p>
<span id="more"></span>

<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>传统的编码器解码器一般使用RNN，这也是在机器翻译中最经典的模型，但RNN本身也存在局限，这类模型的发展大多从三个方面入手：</p>
<ul>
<li>input的方向性：单向或双向</li>
<li>深度： 单层或多层</li>
<li>类型：RNN，LSTM或GRU</li>
</ul>
<p>由于无论输入如何变化，encoder给出的都是一个固定维数的向量，存在信息损失；在生成文本时，生成每个词所用到的语义向量都是一样的，这显然过于简单。</p>
<p>为解决上述问题，引入Transformer的概念，整个网络结构完全是由Attention机制组成，Attention机制是将单个序列的不同位置联系起来计算序列表示的一种注意机制。</p>
<p><img src="https://whh.plus/images/attention.jpg" alt="Attention" loading="lazy"></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><blockquote>
<p>Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</p>
</blockquote>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>先看看总体的结构(以论文中的机器翻译为例)</p>
<p><img src="https://whh.plus/images/theTransformer.png" alt="Transformer" loading="lazy"></p>
<p>Transformer本质是一个Encoder-Decoder的结构</p>
<p><img src="https://whh.plus/images/Thetransformer1.png" alt="Transformer1" loading="lazy"></p>
<p>编码器和解码器都是由六个部分组成的，编码器的输出作为解码器的输入</p>
<p><img src="https://whh.plus/images/Thetransformer2.png" alt="Transformer2" loading="lazy"></p>
<p>每个encoder中有self-attention和feed forward neural network，数据先经过self-attention模块，得到一个加权之后的特征向量$Z$，即$Attention(Q,K,V)$</p>
<p>$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</p>
<p>然后送入前馈神经网络，前馈神经网络有两层，第一层的激活函数是Relu，第二层的激活函数为线性激活函数，表示为：<br>$$FFN(Z)=max(0,ZW_1+b_1)W_2+b_2$$</p>
<p>Encoder结构如下：<br><img src="https://whh.plus/images/encoder.png" alt="encoder" loading="lazy"></p>
<p>Decoder的结构，相较于Encoder，多了个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值。</p>
<ul>
<li>Self-Attention : 当前翻译和已经翻译的前文之间的关系</li>
<li>Encoder-Decoder Attention : 当前翻译和编码的特征向量之间的关系</li>
</ul>
<p>Decoder结构如下：<br><img src="https://whh.plus/images/decoder.png" alt="decoder" loading="lazy"></p>
<h3 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码"></a>输入编码</h3><p>上面说明的是主要的网络框架，现在介绍数据输入。一般使用嵌入算法把每个输入字转化成向量<br><img src="https://whh.plus/images/embedding.png" alt="embedding" loading="lazy"></p>
<p>词嵌入的维度$d_{model}=512$，嵌入单词到输入序列，每个单词都会输入到每个编码器的两层<br><img src="https://whh.plus/images/encoder1.png" alt="encoder" loading="lazy"><br>Transform的关键特性，每个位置的词仅流过自己的编码器路径。self-attention中这些路径两两之间相互依赖，但前馈层没有这些依赖，因此各种路径在流过前馈层时并行执行。</p>
<h4 id="编码过程"><a href="#编码过程" class="headerlink" title="编码过程"></a>编码过程</h4><p>一个向量序列作为Encoder的输入，将向量传入self-attention处理，进入FFNN，然后再将输出向上传到下一个Encoder<br><img src="https://whh.plus/images/encoder2.png" alt="encoder" loading="lazy"></p>
<h3 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h3><p>self-attention是Transform最核心内容。Attention的核心是<strong>输入向量的每一个单词学习一个权重</strong></p>
<p>例如：</p>
<blockquote>
<p>The animal didn’t cross the street because it was too tired</p>
</blockquote>
<p>加权后得到类似如下加权情况：</p>
<p><img src="https://whh.plus/images/attention1.png" alt="attention" loading="lazy"></p>
<p>当模型处理单词“it”时，self-attention允许将“it”和“animal”联系起来。当模型处理每个位置的词时，self-attention允许模型将句子的其他位置信息作为辅助线索来更好的编码当前词。</p>
<p>当编码“it”时(编码器的最后层输出)，部分attention集中于“the animal”，并将其表示合并进入到“it”的编码中</p>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Tensor2Tensor notebook</a></p>
<h4 id="self-attention细节"><a href="#self-attention细节" class="headerlink" title="self-attention细节"></a>self-attention细节</h4><p>根据编码器的输入向量。生成三个向量，query-vec,key-vec，value-vec，生成方法为分别乘以三个矩阵，这些矩阵在训练过程中需要学习。【不是每个词向量独享3个矩阵，而是所有输入共享3个转换矩阵；权重矩阵是基于输入位置的转换矩阵；】新向量的维度比输入词向量的维度要小(512-&gt;64)，并不是必须要小，是为了让多头attention的计算更稳定<br><img src="https://whh.plus/images/QKV.png" alt="QKV" loading="lazy"></p>
<p>$$Query_{vec}=W_Q*q_1$$</p>
<p>$$Key_{vec}=W_K*k_1$$</p>
<p>$$Values_{vec}=W_V*v_1$$</p>
<p><img src="https://whh.plus/images/QKV1.png" alt="QKV" loading="lazy"></p>
<p>Attention计算过程</p>
<ul>
<li>将单词转化为嵌入向量(Embedding)</li>
<li>根据嵌入向量得到三个输入向量$q,k,v$</li>
<li>为每个向量计算一个score，$Q$与所有词的$K$以此点积得到</li>
<li>为了梯度的稳定，Transformer使用score归一化，即$/\sqrt{d_k}$</li>
<li>对score进行softmax激活函数</li>
<li>softmax点乘value值$v$，得到加权的每个输入向量评分$v$(scaled Dot-Product Attention)</li>
<li>相加之后得到最终输入结果</li>
</ul>
<p><img src="https://whh.plus/images/computeAttention.png" alt="Self-Attention计算示例图" loading="lazy"><br>$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V $$</p>
<p><img src="https://whh.plus/images/softmax.png" alt="Self-Attention矩阵表示" loading="lazy"></p>
<h4 id="残差"><a href="#残差" class="headerlink" title="残差"></a>残差</h4><p>在self-attention需要强调的最后一点是采用了残差网络中的short-cut结构，目的是为了解决深度学习中的退化问题。</p>
<p><img src="https://whh.plus/images/short-cut.png" alt="Self-Attention中的short-cut连接" loading="lazy"></p>
<p>向量和self-attention的相关图层可视化如下：</p>
<p><img src="https://whh.plus/images/short-cut1.png" alt="Self-Attention中的short-cut连接" loading="lazy"></p>
<p>Transformer可视化如下图：</p>
<p><img src="https://whh.plus/images/transformer1.png" alt="Transformer" loading="lazy"></p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p>Multi-Head Attention相当于8个不同的self-attention的集成</p>
<ul>
<li>将数据X分别输入到8个self-attention中，得到8个加权后的特征矩阵<br><img src="https://whh.plus/images/multi-head1.png" alt="第一步" loading="lazy"></li>
<li>将8个矩阵按列拼成一个大的特征矩阵<br><img src="https://whh.plus/images/multi-head2.png" alt="第二步" loading="lazy"></li>
<li>特征矩阵经过一层全连接后得到输出$Z$</li>
</ul>
<p>整个过程：</p>
<p><img src="https://whh.plus/images/multiAttention.png" alt="Multi-Head Attention" loading="lazy"></p>
<p>同Self-Attention一样，Mukti-Head Attention也加入了short-cut机制</p>
<p>现在加入Attention heads后，重新看下当编码“it”时，哪些attention head会被集中<br><img src="https://whh.plus/images/multi-head3.png" alt="it" loading="lazy"></p>
<p>编码“it”时，一个attention head集中于“the animal”，另一个head集中于“tired”</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Transformer模型没有捕捉顺序序列的能力，无论句子怎么打乱，得到的结果都是类似的。为了解决这个问题，论文提出位置编码概念。</p>
<p>位置编码常见模式有两种：A.根据数据学习 B.自己设计编码规则。论文作者采用第二种方式，通常位置编码的长度为$d_{model}$的特征向量，便于和词向量进行单位相加<br><img src="https://whh.plus/images/position.png" alt="位置编码" loading="lazy"></p>
<p>$$PE(pos,2i)=sin\frac{pos}{10000^{\frac{2i}{d_{model}}}}$$</p>
<p>$$PE(pos,2i+1)=cos\frac{pos}{10000^{\frac{2i}{d_{model}}}}$$</p>
<p>$pos$ : 单词的位置</p>
<p>$i$ : 单词的维度</p>
<p><img src="https://whh.plus/images/transformer.png" alt="Transformer" loading="lazy"><em>模型架构图</em></p>
<p><strong>参考文献</strong><br>1.<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75591049?from_voters_page=true">The Illustrated Transformer</a><br>2.<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhanghaiyan/p/11079504.html">Attention is all you need-详解Transformer</a></p>
</div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">I'm so cute. Please give me money.</div><div id="qr" style="display:none;"><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/alipay-qrcode.jpg"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/alipay-qrcode.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/qqpay-qrcode.png"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/qqpay-qrcode.png" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/wechatpay-qrcode.jpg"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/wechatpay-qrcode.jpg" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>先逃跑再说</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://wentianhao.github.io/2020/10/10/Attention/" title="Attention Is All You Need">https://wentianhao.github.io/2020/10/10/Attention/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/10/20/top/" rel="prev" title="linux命令：top"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">linux命令：top</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/10/09/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8Cpython%E8%84%9A%E6%9C%AC/" rel="next" title="服务器后台运行python脚本"><span class="post-nav-text">服务器后台运行python脚本</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>要不要和我说些什么？</span><br></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 先逃跑再说</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v5.4.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.1</span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></div></body></html>